# Data Quality ETL Pipeline - Organization & Person Data Consolidation

A comprehensive ETL pipeline for cleaning and consolidating customer data from WH_PERS (person) and WH_ORG (organization) tables, generating unified snapshots of customer records linked to active accounts.

## ğŸ—ï¸ Project Structure

```
data_quality/
â”œâ”€â”€ data/                           # Data directories with placeholder files
â”‚   â”œâ”€â”€ inputs/                     # Drop zone for Excel files to process
â”‚   â”‚   â”œâ”€â”€ org/                    # Organization files with additional notes
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md           # Instructions for org input files
â”‚   â”‚   â”‚   â””â”€â”€ SAMPLE_FORMAT.txt   # Example format (delete before use)
â”‚   â”‚   â””â”€â”€ pers/                   # Person files with additional notes
â”‚   â”‚       â”œâ”€â”€ README.md           # Instructions for person input files
â”‚   â”‚       â””â”€â”€ SAMPLE_FORMAT.txt   # Example format (delete before use)
â”‚   â”œâ”€â”€ archive/                    # Processed files automatically moved here
â”‚   â”‚   â””â”€â”€ README.md               # Archive folder documentation
â”‚   â””â”€â”€ README.md                   # Data directory overview
â”œâ”€â”€ outputs/                        # Generated reports and processed data
â”‚   â””â”€â”€ README.md                   # Outputs documentation
â”œâ”€â”€ notebooks/                      # Jupyter notebooks for analysis
â”‚   â””â”€â”€ code.ipynb                  # Development and exploration notebook
â”œâ”€â”€ src/                           # Core processing logic
â”‚   â”œâ”€â”€ main.py                     # CLI entry point
â”‚   â””â”€â”€ data_quality/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ core.py                 # ETL functions
â”œâ”€â”€ tests/                         # Comprehensive test suite
â”‚   â”œâ”€â”€ test_core.py               # 50+ tests for all functions
â”‚   â””â”€â”€ test_dtype_fixes.py        # Robust dtype conversion testing
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ data_sources.md            # Database schema and table definitions
â”‚   â””â”€â”€ NOTES.md                   # Development notes
â”œâ”€â”€ .github/                       # GitHub configuration
â”‚   â””â”€â”€ copilot-instructions.md    # AI agent guidance
â”œâ”€â”€ pyproject.toml                 # Project configuration & dependencies
â”œâ”€â”€ uv.lock                        # Dependency lock file
â””â”€â”€ README.md                      # This file
```

## ğŸš€ Quick Start

### 1. Clone & Setup
```bash
git clone <repository-url>
cd data_quality
uv sync                           # Install dependencies
```

### 2. Ready-to-Use Structure
The repository includes placeholder files in all data directories:
- `data/inputs/org/` - Ready for organization Excel files
- `data/inputs/pers/` - Ready for person Excel files  
- `data/archive/` - Auto-populated after processing
- `outputs/` - Auto-populated with results

### 3. Process Input Files (Optional)
```bash
# Drop Excel files in data/inputs/org/ or data/inputs/pers/
# Then run:
./.venv/bin/python -m src.main
```

### 4. Run Tests
```bash
./.venv/bin/python -m pytest tests/ -v
```

## ğŸ“‚ Data Processing Workflow

1. **Input Files**: Drop Excel files in `data/inputs/org/` or `data/inputs/pers/`
2. **Processing**: Run pipeline to merge with your processed data  
3. **Auto-Archive**: Input files automatically moved to `data/archive/`
4. **Results**: Final datasets saved to `outputs/` with timestamps

## ğŸ“‹ Input File Requirements

### Organization Files (`data/inputs/org/`)
- Excel format (.xlsx or .xls)
- **Required**: `ORGNBR` column (uppercase)
- **Additional**: Any extra columns (NOTES, CONTACT_PERSON, etc.)
- One file at a time

### Person Files (`data/inputs/pers/`)
- Excel format (.xlsx or .xls) 
- **Required**: `PERSNBR` column (uppercase)
- **Additional**: Any extra columns (COMMENTS, PRIORITY, etc.)
- One file at a time

## ğŸ”§ Core Functions

Located in `src/data_quality/core.py`:

- `create_org_table_with_address()` - Merges WH_ORG â†’ ORGADDRUSE â†’ WH_ADDR
- `create_pers_table_with_address()` - Merges WH_PERS â†’ PERSADDRUSE â†’ WH_ADDR  
- `filter_to_active_accounts()` - Filters to customers with active accounts
- `merge_with_input_file()` - Merges processed data with input Excel files
- `archive_input_file()` - Auto-archives processed files with original names

## ğŸ§ª Testing

### Comprehensive Test Suite
Located in `tests/` with 50+ tests covering all functionality:

**Core ETL Tests** (`test_core.py`):
- âœ… Happy path scenarios
- âœ… Edge cases and error conditions  
- âœ… Field filtering and validation
- âœ… Input file processing and archiving
- âœ… Path object support

**Robust Dtype Conversion Tests** (`test_dtype_fixes.py`):
- âœ… **Mixed dtype scenarios**: int64 â†” float64 â†” object conversions
- âœ… **Decimal handling**: 1.7 â†’ "1", 123.0 â†’ "123" 
- âœ… **NaN preservation**: Nullable integers and missing values
- âœ… **Large numbers**: 999,999,999,999+ values
- âœ… **Edge cases**: Empty DataFrames, special characters
- âœ… **Input file merging**: All dtype combinations work seamlessly

### Running Tests
```bash
# Run all tests
uv run pytest tests/ -v

# Run specific test categories
uv run pytest tests/test_core.py -v                    # Core ETL functionality
uv run pytest tests/test_dtype_fixes.py -v             # Dtype conversion robustness

# Run comprehensive dtype testing standalone
./.venv/bin/python tests/test_dtype_fixes.py           # Full test report with summary
```

The dtype conversion fixes ensure your ETL pipeline handles real-world database inconsistencies where:
- Organization IDs might be `int64` in one table, `float64` in another
- Account numbers could be stored as strings `"1001"` or floats `1001.0`  
- Missing values create nullable integer types that break standard joins

**Result**: Zero failed joins due to dtype mismatches! ğŸ‰

## ğŸ“š Documentation

- **Database Schema**: See `docs/data_sources.md` for complete table definitions
- **AI Instructions**: See `.github/copilot-instructions.md` for development patterns
- **Development Notes**: See `docs/NOTES.md` for workflow details

## ğŸ¯ Next Steps

1. **Implement Database Functions**:
   - `create_acct_df()` - Active accounts snapshot
   - `load_database_tables()` - Database connectivity

2. **Production Deployment**:
   - Configure database connections
   - Test with real data
   - Schedule automated runs

## ğŸ› ï¸ Technology Stack

- **Python 3.12+** with uv dependency management
- **pandas** for data manipulation
- **pytest** for testing
- **openpyxl** for Excel file handling
- **VS Code** with Jupyter notebook support
