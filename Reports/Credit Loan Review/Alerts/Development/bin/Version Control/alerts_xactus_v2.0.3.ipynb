{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Portfolio Alerts\n",
    "# Developed by CD\n",
    "# v2.0.1\n",
    "\n",
    "from io import StringIO\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta, date\n",
    "from sqlalchemy.ext.asyncio import create_async_engine\n",
    "from sqlalchemy import text\n",
    "from typing import List\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from cryptography.fernet import Fernet\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import sys\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "\n",
    "def retrieve_data():\n",
    "    \"\"\"\n",
    "    Retrieve data from COCC database\n",
    "    \"\"\"\n",
    "    class DatabaseHandler:\n",
    "        \"\"\"\n",
    "        This class abstracts the connection to the database and allows a clean\n",
    "        interface for the developer to use.\n",
    "\n",
    "        This connector can handle async queries\n",
    "\n",
    "        \"\"\"\n",
    "        def __init__(self, tns_admin_path):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                tns_admin_path (str): Oracle driver path\n",
    "                credentials_path_db1 (str): Database 1 credentials path\n",
    "                credentials_path_db1 (str): Databsae 2 credentials path\n",
    "            \"\"\"\n",
    "            os.environ['TNS_ADMIN'] = tns_admin_path\n",
    "            \n",
    "            # Load private key\n",
    "            key_key_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\key.key'\n",
    "            with open(key_key_path, \"rb\") as key_file:\n",
    "                key = key_file.read()\n",
    "\n",
    "            cipher = Fernet(key)\n",
    "            \n",
    "            # Load encrypted data\n",
    "            encoded_env_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\.env.enc'\n",
    "            with open(encoded_env_path, \"rb\") as encrypted_file:\n",
    "                encrypted_data = encrypted_file.read()\n",
    "\n",
    "            decrypted_data = cipher.decrypt(encrypted_data).decode()\n",
    "\n",
    "            env_file = StringIO(decrypted_data)\n",
    "            load_dotenv(stream=env_file)\n",
    "\n",
    "            self.username1 = os.getenv('main_username')\n",
    "            self.password1 = os.getenv('main_password')\n",
    "            self.dsn1 = os.getenv('main_dsn')\n",
    "\n",
    "            self.username2 = os.getenv('datamart_username')\n",
    "            self.password2 = os.getenv('datamart_password')\n",
    "            self.dsn2 = os.getenv('datamart_dsn')\n",
    "\n",
    "            self.connection_string1 = f'oracle+oracledb://{self.username1}:{self.password1}@{self.dsn1}'\n",
    "            self.connection_string2 = f'oracle+oracledb://{self.username2}:{self.password2}@{self.dsn2}'\n",
    "\n",
    "            self.engine1 = create_async_engine(self.connection_string1, max_identifier_length=128, echo=False, future=True)\n",
    "            self.engine1.dialect.hide_parameters = True\n",
    "            self.engine2 = create_async_engine(self.connection_string2, max_identifier_length=128, echo=False, future=True)\n",
    "            self.engine1.dialect.hide_parameters = True\n",
    "\n",
    "\n",
    "        async def query(self, sql_query, engine=1):\n",
    "            \"\"\"\n",
    "            This allows abstraction of the connection and the class\n",
    "            so the developer can query a single table as a dataframe\n",
    "\n",
    "            Args:\n",
    "                sql_query (str): The query to SQL database is passed as a string\n",
    "                engine (int): This selects the database. There are two engines:\n",
    "                    1 -> R1625\n",
    "                    2 -> COCC DataMart\n",
    "\n",
    "            Returns:\n",
    "                df: The SQL query is returned as a pandas DataFrame\n",
    "\n",
    "            Usage:\n",
    "                df = db_handler.query(\"SELECT * FROM DB.TABLE\", engine=1)\n",
    "\n",
    "                In this example, db_handler = DatabaseHandler(args)\n",
    "            \"\"\"\n",
    "            if engine == 1:\n",
    "                selected_engine = self.engine1\n",
    "            elif engine == 2:\n",
    "                selected_engine = self.engine2\n",
    "            else:\n",
    "                raise ValueError(\"Engine must be 1 or 2\")\n",
    "\n",
    "            async with selected_engine.connect() as connection:\n",
    "                result = await connection.execute(sql_query)\n",
    "                rows = result.fetchall()\n",
    "                if not rows:\n",
    "                    return pd.DataFrame()\n",
    "                df = pd.DataFrame(rows, columns=result.keys())\n",
    "            return df\n",
    "\n",
    "        async def close(self):\n",
    "            if self.engine1:\n",
    "                await self.engine1.dispose()\n",
    "            if self.engine2:\n",
    "                await self.engine2.dispose()\n",
    "\n",
    "\n",
    "    # Database Connection Configuration\n",
    "    tns_admin_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\tns_admin'\n",
    "    db_handler = DatabaseHandler(tns_admin_path)\n",
    "\n",
    "    async def fetch_data(queries):\n",
    "        try:\n",
    "            tasks = {query['key']: asyncio.create_task(db_handler.query(query['sql'], query['engine'])) for query in queries}\n",
    "            results = await asyncio.gather(*tasks.values())\n",
    "            return {key: df for key, df in zip(tasks.keys(), results)}\n",
    "        except Exception as e:\n",
    "            print(f\"Error\")\n",
    "            raise\n",
    "        finally:\n",
    "            await db_handler.close()\n",
    "\n",
    "    def run_sql_queries():\n",
    "        # lookup table\n",
    "        # Engine 1\n",
    "        lookup_df = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            sys.all_tab_columns col\n",
    "        \"\"\")\n",
    "\n",
    "        # acctcommon\n",
    "        # engine 1\n",
    "        acctcommon = text(\"\"\"\n",
    "        SELECT \n",
    "            a.ACCTNBR, \n",
    "            a.EFFDATE, \n",
    "            a.MJACCTTYPCD, \n",
    "            a.PRODUCT, \n",
    "            a.CURRMIACCTTYPCD, \n",
    "            a.BOOKBALANCE, \n",
    "            a.LOANOFFICER, \n",
    "            a.OWNERNAME, \n",
    "            a.CURRACCTSTATCD, \n",
    "            a.CONTRACTDATE, \n",
    "            a.NOTEBAL\n",
    "        FROM \n",
    "            OSIBANK.WH_ACCTCOMMON a\n",
    "        WHERE \n",
    "            a.CURRACCTSTATCD IN ('ACT')\n",
    "        \"\"\")\n",
    "\n",
    "        acctloan = text(\"\"\"\n",
    "        SELECT \n",
    "            a.ACCTNBR, \n",
    "            a.COBAL, \n",
    "            a.CREDITLIMITAMT, \n",
    "            a.RISKRATINGCD, \n",
    "            a.TOTALPCTSOLD, \n",
    "            a.CREDLIMITCLATRESAMT\n",
    "        FROM \n",
    "            OSIBANK.WH_ACCTLOAN a\n",
    "        \"\"\")\n",
    "\n",
    "        loans = text(\"\"\"\n",
    "        SELECT \n",
    "            a.ACCTNBR, \n",
    "            a.AVAILBALAMT\n",
    "        FROM \n",
    "            OSIBANK.WH_LOANS a\n",
    "        \"\"\")\n",
    "\n",
    "        househldacct = text(\"\"\"\n",
    "        SELECT \n",
    "            a.HOUSEHOLDNBR, \n",
    "            a.ACCTNBR\n",
    "        FROM \n",
    "            OSIEXTN.HOUSEHLDACCT a\n",
    "        \"\"\")\n",
    "\n",
    "        allroles = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            OSIBANK.WH_ALLROLES a\n",
    "        WHERE\n",
    "            a.ACCTROLECD IN ('GUAR')\n",
    "        \"\"\")\n",
    "\n",
    "        persaddruse = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            OSIBANK.PERSADDRUSE\n",
    "        \"\"\")\n",
    "\n",
    "        wh_addr = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            OSIBANK.WH_ADDR\n",
    "        \"\"\")\n",
    "\n",
    "        pers = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            OSIBANK.PERS\n",
    "        \"\"\")\n",
    "\n",
    "        acctstatistichist = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            OSIBANK.ACCTSTATISTICHIST\n",
    "        \"\"\")\n",
    "\n",
    "        acctloanlimithist = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            OSIBANK.ACCTLOANLIMITHIST\n",
    "        \"\"\")\n",
    "\n",
    "        queries = [\n",
    "            {'key':'acctcommon', 'sql':acctcommon, 'engine':1},\n",
    "            {'key':'acctloan', 'sql':acctloan, 'engine':1},\n",
    "            {'key':'loans', 'sql':loans, 'engine':1},\n",
    "            {'key':'househldacct', 'sql':househldacct, 'engine':1},\n",
    "            {'key':'allroles', 'sql':allroles, 'engine':1},\n",
    "            {'key':'persaddruse', 'sql':persaddruse, 'engine':1},\n",
    "            {'key':'wh_addr', 'sql':wh_addr, 'engine':1},\n",
    "            {'key':'pers', 'sql':pers, 'engine':1},\n",
    "            {'key':'acctstatistichist', 'sql':acctstatistichist, 'engine':1},\n",
    "            {'key':'acctloanlimithist', 'sql':acctloanlimithist, 'engine':1},\n",
    "        ]\n",
    "\n",
    "        async def run_queries():\n",
    "            return await fetch_data(queries)\n",
    "        \n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            return loop.run_until_complete(run_queries())\n",
    "        else:\n",
    "            return asyncio.run(run_queries())\n",
    "        \n",
    "    data = run_sql_queries()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "#################################\n",
    "# Core ETL\n",
    "\n",
    "def filter_and_merge_loan_tables(acctcommon, acctloan, loans):\n",
    "    \"\"\"\n",
    "    This filters on CML Loans & merges tables to consolidate loan data.\n",
    "    Data cleansing on numeric fields is performed.\n",
    "    \n",
    "    Args:\n",
    "        acctcommon: WH_ACCTCOMMON\n",
    "        acctloan: WH_ACCTLOAN\n",
    "        loans: WH_LOANS\n",
    "        \n",
    "    Returns:\n",
    "        df: Consolidated loan data as a dataframe\n",
    "        \n",
    "    Operations:\n",
    "        - mjaccttypcd (Major) == 'CML'\n",
    "        - left merge of df (acctcommon) & acctloan on 'acctnbr'\n",
    "        - left merge of df & loans on 'acctnbr'\n",
    "        - drop all fields that are completely null/empty\n",
    "        - Replace null/na values with 0 for numeric fields:\n",
    "            - total pct sold\n",
    "            - avail bal amt\n",
    "            - credit limit collateral reserve amt\n",
    "        - loans with risk rating 4 or 5 are excluded\n",
    "    \"\"\"\n",
    "\n",
    "    # CML loans\n",
    "    df = acctcommon[acctcommon['mjaccttypcd'].isin(['CML'])]\n",
    "\n",
    "    # Merging and dropping blank fields\n",
    "    df = pd.merge(df, acctloan, on='acctnbr', how='left', suffixes=('_df', '_acctloan'))\n",
    "    df = pd.merge(df, loans, on='acctnbr', how='left', suffixes=('_df', '_loans'))\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Data Cleansing\n",
    "    df['totalpctsold'] = df['totalpctsold'].fillna(0)\n",
    "    df['availbalamt'] = df['availbalamt'].fillna(0)\n",
    "    df['credlimitclatresamt'] = df['credlimitclatresamt'].fillna(0)\n",
    "    df = df[~df['riskratingcd'].isin(['4','5'])]\n",
    "    df = df[~df['curracctstatcd'].isin(['NPFM'])] # This is handled by SQL query normally\n",
    "    \n",
    "    # Unit test\n",
    "    assert not df['curracctstatcd'].isin(['NPFM']).any(), \"NPFM loans were not filtered out\"\n",
    "    assert not df['riskratingcd'].isin(['4','5']).any(), \"4 and 5 rated loans were not filtered out\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "def append_total_exposure_field(df):\n",
    "    \"\"\" \n",
    "    Single Obligor Exposure Calculation\n",
    "    \n",
    "    Args:\n",
    "        df: loan_data is loaded in\n",
    "    \n",
    "    Returns:\n",
    "        df: loan_data is returned with new fields appended\n",
    "        \n",
    "    Operations:\n",
    "        bookbalance -> if currmiaccttypcd == 'CM45', use notebal, else bookbalance\n",
    "            - Tax Exempt bonds always have $0 as book balance so adjustment is made\n",
    "        net balance == bookbalance - cobal\n",
    "            - BCSB balance - Charged off amount (COBAL)\n",
    "        net available == available balance amount * (1 - total pct sold)\n",
    "        net collateral reserve == collateral reserve * (1 - total pct sold)\n",
    "        total exposure == net balance + net available + net collateral reserve\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Tax Exempt bonds always have $0 Book Balance so need to take NOTEBAL\n",
    "    df['bookbalance'] = np.where(df['currmiaccttypcd'].isin(['CM45']), df['notebal'], df['bookbalance'])\n",
    "    df['Net Balance'] = df['bookbalance'] - df['cobal']\n",
    "    df['Net Available'] = df['availbalamt'] * (1 - df['totalpctsold'])\n",
    "    df['Net Collateral Reserve'] = df['credlimitclatresamt'] * (1 - df['totalpctsold'])\n",
    "    df['Total Exposure'] = df['Net Balance'] + df['Net Available'] + df['Net Collateral Reserve']\n",
    "    return df\n",
    "\n",
    "def drop_hh_duplicates(df):\n",
    "    \"\"\"\n",
    "    Drop duplicate rows in Household table\n",
    "    \n",
    "    Args:\n",
    "        df: HOUSEHLDACCT table (COCC)\n",
    "        \n",
    "    Returns:\n",
    "        cleaned_df: de-duplicated df\n",
    "        \n",
    "    Operations:\n",
    "        - drop_duplicates(subset='acctnbr', keep='first')\n",
    "    \"\"\"\n",
    "    cleaned_df = df.drop_duplicates(subset='acctnbr', keep='first')\n",
    "    \n",
    "    assert cleaned_df['acctnbr'].duplicated().sum() == 0, \"There are duplicate acctnbrs\" \n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def append_household_number(df, househldacct):\n",
    "    \"\"\"\n",
    "    Append Household Number to Loan Data\n",
    "    \n",
    "    Args:\n",
    "        df: loan_data\n",
    "        househldacct: Household Acct Table from COCC (OSIEXTEN.HOUSEHLDACCT)\n",
    "    \n",
    "    Returns:\n",
    "        df: loan_data with household number appended\n",
    "        \n",
    "    Operations:\n",
    "        - Left merge of df & househldacct table on 'acctnbr'\n",
    "    \"\"\"\n",
    "    df = pd.merge(df, househldacct, on='acctnbr', how='left', suffixes=('_df', '_househldacct'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def household_total_exposure(df):\n",
    "    \"\"\"\n",
    "    Household Total Exposure:\n",
    "    Grouping on household key, the total exposure is summed\n",
    "    \n",
    "    Args:\n",
    "        df: loan_data\n",
    "    \n",
    "    Returns:\n",
    "        household_grouping_df: A new dataframe with 2 columns:\n",
    "            - Householdnbr\n",
    "            - Total Exposure ($)\n",
    "    \n",
    "    Operations:\n",
    "        - Group By: Householdnbr\n",
    "        - Sum: Total Exposure\n",
    "    \n",
    "    \"\"\"\n",
    "    household_grouping_df = df.groupby('householdnbr')['Total Exposure'].sum().reset_index()\n",
    "    household_grouping_df = pd.DataFrame(household_grouping_df)\n",
    "    return household_grouping_df\n",
    "\n",
    "\n",
    "def append_household_exposure(df, household_grouping_df):\n",
    "    \"\"\"\n",
    "    Append household exposure back to loan_data\n",
    "    \n",
    "    Args:\n",
    "        df: loan_data\n",
    "        household_grouping_df: df with household number & total exposure\n",
    "        \n",
    "    Returns:\n",
    "        df: loan data after appending household exposure\n",
    "        \n",
    "    Operations:\n",
    "        - Left merge of df & household_grouping_df on 'householdnbr'\n",
    "        \n",
    "    \"\"\"\n",
    "    df = pd.merge(df, household_grouping_df, on='householdnbr', how='left', suffixes=('_df','_hhgroup'))\n",
    "    return df\n",
    "\n",
    "def filter_to_target_products(df):\n",
    "    \"\"\" \n",
    "    Filtering data down to products within Alerts criteria\n",
    "    \n",
    "    Args:\n",
    "        df: loan_data\n",
    "    \n",
    "    Returns:\n",
    "        df: loan_data after filters are applied to set the scope of Alerts system\n",
    "        \n",
    "    Operations:\n",
    "        - currmiaccttypcd (minor) is in:\n",
    "            - \"CM06\",\"CM11\",\"CM30\",\"CM52\",\"CM57\",\"CM62\",\"CM71\",\"CM76\"\n",
    "        - creditlimitamt <= $500,000\n",
    "        - total household exposure <= $1,000,000\n",
    "    \"\"\"\n",
    "    # Lines of Credit\n",
    "    df = df[df['currmiaccttypcd'].isin([\"CM06\",\"CM11\",\"CM30\",\"CM52\",\"CM57\",\"CM62\",\"CM71\",\"CM76\"])]\n",
    "    # Credit Limit Amount <= $500M & Household Exposure <= $1MM\n",
    "    df = df[(df['creditlimitamt'] <= 500000) & (df['Total Exposure_hhgroup'] <= 1000000)]\n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "# Personal Guarantors extracted\n",
    "def personal_guarantors(allroles, persaddruse, wh_addr, pers):\n",
    "    \"\"\"\n",
    "    Personal Guarantor information is pulled from COCC and several tables are merged.\n",
    "    \n",
    "    Args:\n",
    "        allroles: ALLROLES table (COCC)\n",
    "        persaddruse: PERSADDRUSE table (COCC)\n",
    "        wh_addr: WH_ADDR table (COCC)\n",
    "        pers: WH_PERS table (COCC)\n",
    "        \n",
    "    Returns:\n",
    "        df: Dataframe of personal guarantors\n",
    "        \n",
    "    Operations:\n",
    "        - allroles table where 'acctrolecd' = 'GUAR' (guarantor role)\n",
    "        - allroles where 'persnbr' is not null (this excludes organizations)\n",
    "        - persaddruse where 'addrusecd' == 'PRI' (only primary address is considered)\n",
    "        - left merge of allroles & persaddruse tables on 'persnbr'\n",
    "        - left merge of df (merged df from earlier step) & wh_addr on 'addrnbr'\n",
    "        - left merge of df & pers on 'persnbr'\n",
    "        - filtered out unnecessary fields\n",
    "            - keeping only ['acctnbr','persnbr','firstname','lastname','text1',\n",
    "                            'cityname','statecd','zipcd']\n",
    "    \"\"\"\n",
    "    allroles = allroles[allroles['acctrolecd'] == 'GUAR']\n",
    "    allroles = allroles[allroles['persnbr'].notnull()]\n",
    "    persaddruse = persaddruse[persaddruse['addrusecd'] == \"PRI\"]\n",
    "    # Merge\n",
    "    df = pd.merge(allroles, persaddruse, on='persnbr',how='left', suffixes=('_allroles','_persaddruse'))\n",
    "    df = pd.merge(df, wh_addr, on='addrnbr',how='left', suffixes=('_df','_addr'))\n",
    "    df = pd.merge(df, pers, on='persnbr', how='left', suffixes=('_df','_pers'))\n",
    "    df = df[['acctnbr','persnbr','firstname','lastname','text1','cityname','statecd','zipcd']]\n",
    "    return df\n",
    "\n",
    "def merge_guar_with_loan_data(df, pg_section):\n",
    "    \"\"\"\n",
    "    Merging Loan Data & Personal Guarantor information\n",
    "    \n",
    "    Args:\n",
    "        df: loan_data\n",
    "        pg_section: personal guarantor dataframe\n",
    "    \n",
    "    Returns:\n",
    "        df: loan_data merged with personal guarantor data (inner merge)\n",
    "        \n",
    "    Operations:\n",
    "        - Inner merge of df & pg_section (personal guarantor section) on 'acctnbr'\n",
    "    \"\"\"\n",
    "    df = pd.merge(df, pg_section, on='acctnbr', how='inner', suffixes=('_df','_pg'))\n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "def acctstatistichist_cleaning(df, acctcommon):\n",
    "    \"\"\" \n",
    "    Cleans acctstatistichist table and adds new fields for filtering\n",
    "    \n",
    "    Args:\n",
    "        df: ACCTSTATISTICHIST table (COCC)\n",
    "        acctcommon: WH_ACCTCOMMON table (COCC)\n",
    "            - Used for current date\n",
    "    \n",
    "    Returns:\n",
    "        df: ACCSTATISTICHIST with new calculated date fields\n",
    "            - 'event_date': date (month) of event occurance\n",
    "            - 'current_date': current_date\n",
    "            - 'year_start': First day of year (used for YTD calculations)\n",
    "            - 'year_ago_date': Today's date minus 1 year (for TTM calculations)\n",
    "        \n",
    "    Operations:\n",
    "        - monthcd zero fill 2 digits\n",
    "        - monthcd to string type\n",
    "        - yearnbr to string type\n",
    "        - event_date field = df['yearnbr'] + \"-\" + df['monthcd'] + \"-01\"\n",
    "        - current_date == First record in EFFDATE field from acctcommon table\n",
    "            -> this is appended to the dataframe as 'current_date' column\n",
    "        - year_start = current_date year + '01-01'\n",
    "        - year_ago_date = current_date - 1 year\n",
    "    \"\"\"\n",
    "    df['monthcd'] = df['monthcd'].str.zfill(2)\n",
    "    df['monthcd'] = df['monthcd'].astype(str)\n",
    "    df['yearnbr'] = df['yearnbr'].astype(str)\n",
    "    df['event_date'] = df['yearnbr'] + \"-\" + df['monthcd'] + \"-01\"\n",
    "    df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "    \n",
    "    current_date = acctcommon['effdate'][0]\n",
    "    df['current_date'] = current_date\n",
    "    \n",
    "    df['year_start'] = pd.to_datetime(df['current_date'].dt.year.astype(str) + '-01-01')\n",
    "    df['year_ago_date'] = df['current_date'] - pd.DateOffset(years=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "def count_pd(df):\n",
    "    \"\"\"\n",
    "    This will count past due (15+) flags on the account\n",
    "    \n",
    "    Args:\n",
    "        df: ACCTSTATISTICHIST table (COCC)\n",
    "        \n",
    "    Returns:\n",
    "        merged_df: A dataframe with 3 columns:\n",
    "            - acctnbr\n",
    "            - ytd_pd (count)\n",
    "            - ttm_pd (count)\n",
    "    \n",
    "    Operations:\n",
    "        - ytd_df created where event_date >= year_start date\n",
    "        - ttm_df created where event_date >= year_ago date\n",
    "        - statistictypcd (statistic type code) = 'PD'\n",
    "        - Group by acctnbr, sum statistic count\n",
    "        - rename columns to ytd_pd & ttm_pd\n",
    "        - Outer merge ytd_df & ttm_df on acctnbr\n",
    "        - fill null values with 0\n",
    "    \"\"\"\n",
    "    ytd_df = df[df['event_date'] >= df['year_start']]\n",
    "    ttm_df = df[df['event_date'] >= df['year_ago_date']]\n",
    "    \n",
    "    ytd_df = ytd_df[ytd_df['statistictypcd'].isin(['PD'])]\n",
    "    ttm_df = ttm_df[ttm_df['statistictypcd'].isin(['PD'])]\n",
    "    \n",
    "    # Unit Tests\n",
    "    assert (ytd_df['event_date'] >= ytd_df['year_start']).all(), \"Filtering did not apply correctly\"\n",
    "    assert (ttm_df['event_date'] >= ttm_df['year_ago_date']).all(), \"Filtering did not apply correctly\"\n",
    "    \n",
    "    ytd_df = ytd_df.groupby('acctnbr')['statisticcount'].sum().reset_index()\n",
    "    ttm_df = ttm_df.groupby('acctnbr')['statisticcount'].sum().reset_index()\n",
    "    \n",
    "    ytd_df = ytd_df.rename(columns={'statisticcount':'ytd_pd'})\n",
    "    ttm_df = ttm_df.rename(columns={'statisticcount':'ttm_pd'})\n",
    "    \n",
    "    merged_df = pd.merge(ytd_df, ttm_df, on='acctnbr', how='outer')\n",
    "    merged_df['ytd_pd'] = merged_df['ytd_pd'].fillna(0)\n",
    "    merged_df['ttm_pd'] = merged_df['ttm_pd'].fillna(0)\n",
    "    \n",
    "    # Unit Tests\n",
    "    assert merged_df['ytd_pd'].isnull().sum() == 0, \"There are null values\"\n",
    "    assert merged_df['ttm_pd'].isnull().sum() == 0, \"There are null values\"\n",
    "\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "#################################\n",
    "def count_pd30(df):\n",
    "    \"\"\"\n",
    "    This will count past due (30+) flags on the account\n",
    "    \n",
    "    Args:\n",
    "        df: ACCTSTATISTICHIST table (COCC)\n",
    "        \n",
    "    Returns:\n",
    "        merged_df: A dataframe with 3 columns:\n",
    "            - acctnbr\n",
    "            - ytd_pd30 (count)\n",
    "            - ttm_pd30 (count)\n",
    "    \n",
    "    Operations:\n",
    "        - ytd_df created where event_date >= year_start date\n",
    "        - ttm_df created where event_date >= year_ago date\n",
    "        - statistictypcd (statistic type code) = 'PD30'\n",
    "        - Group by acctnbr, sum statistic count\n",
    "        - rename columns to ytd_pd30 & ttm_pd30\n",
    "        - Outer merge ytd_df & ttm_df on acctnbr\n",
    "        - fill null values with 0\n",
    "    \"\"\"\n",
    "    ytd_df = df[df['event_date'] >= df['year_start']]\n",
    "    ttm_df = df[df['event_date'] >= df['year_ago_date']]\n",
    "    \n",
    "    ytd_df = ytd_df[ytd_df['statistictypcd'].isin(['PD30'])]\n",
    "    ttm_df = ttm_df[ttm_df['statistictypcd'].isin(['PD30'])]\n",
    "    \n",
    "    # Unit Tests\n",
    "    assert (ytd_df['event_date'] >= ytd_df['year_start']).all(), \"Filtering did not apply correctly\"\n",
    "    assert (ttm_df['event_date'] >= ttm_df['year_ago_date']).all(), \"Filtering did not apply correctly\"\n",
    "    \n",
    "    ytd_df = ytd_df.groupby('acctnbr')['statisticcount'].sum().reset_index()\n",
    "    ttm_df = ttm_df.groupby('acctnbr')['statisticcount'].sum().reset_index()\n",
    "    \n",
    "    ytd_df = ytd_df.rename(columns={'statisticcount':'ytd_pd30'})\n",
    "    ttm_df = ttm_df.rename(columns={'statisticcount':'ttm_pd30'})\n",
    "    \n",
    "    merged_df = pd.merge(ytd_df, ttm_df, on='acctnbr', how='outer')\n",
    "    merged_df['ytd_pd30'] = merged_df['ytd_pd30'].fillna(0)\n",
    "    merged_df['ttm_pd30'] = merged_df['ttm_pd30'].fillna(0)\n",
    "    \n",
    "    # Unit Tests\n",
    "    assert merged_df['ytd_pd30'].isnull().sum() == 0, \"There are null values\"\n",
    "    assert merged_df['ttm_pd30'].isnull().sum() == 0, \"There are null values\"\n",
    "\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "#################################\n",
    "def append_pd_info(loan_data, pd_df, pd30_df):\n",
    "    \"\"\"\n",
    "    Appending past due and past due 30 counts to loan data\n",
    "    \n",
    "    Args:\n",
    "        loan_data: filtered down loan_data\n",
    "        pd_df: past due 15 days data\n",
    "        pd30_df: past due 30 days data\n",
    "        \n",
    "    Returns:\n",
    "        df: loan_data, with appended past due and past due 30 counts\n",
    "    \n",
    "    Operations:\n",
    "    \"\"\"\n",
    "    df = pd.merge(loan_data, pd_df, on='acctnbr', how='left')\n",
    "    df = pd.merge(df, pd30_df, on='acctnbr', how='left')\n",
    "    \n",
    "    df['ytd_pd'] = df['ytd_pd'].fillna(0)\n",
    "    df['ttm_pd'] = df['ttm_pd'].fillna(0)\n",
    "    df['ytd_pd30'] = df['ytd_pd30'].fillna(0)\n",
    "    df['ttm_pd30'] = df['ttm_pd30'].fillna(0)\n",
    "    \n",
    "    assert df['ytd_pd'].isnull().sum() == 0, \"There are null values\"\n",
    "    assert df['ttm_pd'].isnull().sum() == 0, \"There are null values\"\n",
    "    assert df['ytd_pd30'].isnull().sum() == 0, \"There are null values\"\n",
    "    assert df['ttm_pd30'].isnull().sum() == 0, \"There are null values\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "def deposit_criteria_testing():\n",
    "    \"\"\"\n",
    "    Consolidates deposits by household and calculates deposit change (%) over trailing 12 months\n",
    "        \n",
    "    Returns:\n",
    "        grouped_df: deposit dataframe with deposit change over time and count of overdrafts for each household\n",
    "        \n",
    "    Operations:\n",
    "        - Access daily deposit update from Excel file on DA-1 drive\n",
    "        - Fill null values with 0 for columns:\n",
    "            - NOTEBAL\n",
    "            - Year Ago Balance\n",
    "            - TTM Overdrafts\n",
    "        - Group by household number and sum NOTEBAL, Year Ago Balance, and TTM Overdrafts\n",
    "        - Deposit Change Pct = (NOTEBAL/Year Ago Balance) - 1\n",
    "        - Renamed HOUSEHOLDNBR field to match loan_data householdnbr field\n",
    "    \"\"\"\n",
    "    deposit_file_path = r'\\\\10.161.85.66\\Home\\Share\\Line of Business_Shared Services\\Commercial Credit\\Deposits\\DailyDeposit\\DailyDeposit.xlsx'\n",
    "    deposit_data = pd.read_excel(deposit_file_path, engine='openpyxl')\n",
    "    \n",
    "    deposit_data['NOTEBAL'].fillna(0)\n",
    "    deposit_data['Year Ago Balance'].fillna(0)\n",
    "    deposit_data['TTM Overdrafts'].fillna(0)\n",
    "\n",
    "    grouped_df = deposit_data.groupby('HOUSEHOLDNBR').agg({\n",
    "        'NOTEBAL':'sum',\n",
    "        'Year Ago Balance':'sum',\n",
    "        'TTM Overdrafts':'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    grouped_df['Deposit Change Pct'] = (grouped_df['NOTEBAL']/grouped_df['Year Ago Balance']) - 1\n",
    "    grouped_df = grouped_df.rename(columns={'HOUSEHOLDNBR':'householdnbr'})\n",
    "    \n",
    "    return grouped_df \n",
    "\n",
    "\n",
    "#################################\n",
    "def append_deposit_data(loan_data, deposit_data):\n",
    "    \"\"\"\n",
    "    Append deposit criteria to the loan data\n",
    "    \n",
    "    Args:\n",
    "        loan_data: loan data\n",
    "        deposit_data: deposit data aggregated to household\n",
    "        \n",
    "    Returns:\n",
    "        merged_df: loan_data with deposit data appended\n",
    "        \n",
    "    Operations:\n",
    "        - left merge with loan_data & deposit data on householdnbr\n",
    "    \n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(loan_data, deposit_data, on='householdnbr', how='left')\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "#################################\n",
    "def line_utilization_fetch(loan_data):\n",
    "    \"\"\"\n",
    "    This function gathers line utilization data over past 12 months for each item\n",
    "    \n",
    "    Args:\n",
    "        db_handler: Database Connection abstraction for SQL query\n",
    "        loan_data: loan_data is passed in for unique account numbers\n",
    "        \n",
    "    Returns:\n",
    "        df1: Line Utilization\n",
    "            - 2 column table with:\n",
    "                - acctnbr\n",
    "                - avg line utilization over trailing 12 months\n",
    "            \n",
    "        df2: 30 Day Cleanup Provision\n",
    "            - 2 column table with:\n",
    "                - acctnbr\n",
    "                - cleanup (1=Fail, 0=Pass)\n",
    "    Operations:\n",
    "        - unique account numbers extracted from loan_data\n",
    "        - current_date and year_ago_date are calculated from loan_data (effdate)\n",
    "        - SQL Query:\n",
    "            SELECT b.ACCTNBR, b.EFFDATE, b.BOOKBALANCE, c.CREDITLIMITAMT\n",
    "            FROM COCCDM.WH_ACCTCOMMON b\n",
    "            JOIN COCCDM.WH_ACCTLOAN c\n",
    "            ON b.ACCTNBR = c.ACCTNBR AND b.EFFDATE = c.EFFDATE\n",
    "            WHERE b.ACCTNBR IN ({acctnbr_placeholder})\n",
    "            AND b.EFFDATE BETWEEN TO_DATE('{year_ago_date}', 'yyyy-mm-dd hh24:mi:ss') AND TO_DATE('{current_date}', 'yyyy-mm-dd hh24:mi:ss')\n",
    "        - if creditlimitamt is null, replace with 0\n",
    "        - Calculate line utilization:\n",
    "            - ttm line utilization = bookbalance / creditlimit amount\n",
    "            - fill na values with 0 (0/0) and inf with 100 (credit limit = 0, bookbalance > 0)\n",
    "            - group by acctnbr and take average line utilization\n",
    "        - Calculate 30 day cleanup provision:\n",
    "            - sort by acctnbr and effdate in ascending order\n",
    "            - create a rolling 30 day window and adjust slider through full date range for each\n",
    "            acctnbr\n",
    "            - return 0 if it was paid to 0 for at least 30 days in past year, else return 1 (fail)\n",
    "    \n",
    "    \"\"\"\n",
    "    acctnbrs = loan_data['acctnbr'].unique().tolist()\n",
    "    acctnbr_placeholder = ', '.join([f\"'{acct}'\" for acct in acctnbrs])\n",
    "    \n",
    "    current_date = loan_data['effdate'][0]\n",
    "    temp_data = {\n",
    "        'current_date': [current_date]\n",
    "    }\n",
    "\n",
    "    temp_df = pd.DataFrame(temp_data)\n",
    "    temp_df\n",
    "\n",
    "    temp_df['year_ago_date'] = temp_df['current_date'] - pd.DateOffset(years=1)\n",
    "\n",
    "    current_date = temp_df['current_date'][0].strftime('%Y-%m-%d')+' 00:00:00'\n",
    "    year_ago_date = temp_df['year_ago_date'][0].strftime('%Y-%m-%d')+' 00:00:00'\n",
    "    \n",
    "    def retrieve_data():\n",
    "\n",
    "        class DatabaseHandler:\n",
    "            \"\"\"\n",
    "            This class abstracts the connection to the database and allows a clean\n",
    "            interface for the developer to use.\n",
    "\n",
    "            This connector can handle async queries\n",
    "\n",
    "            \"\"\"\n",
    "            def __init__(self, tns_admin_path):\n",
    "                \"\"\"\n",
    "                Args:\n",
    "                    tns_admin_path (str): Oracle driver path\n",
    "                    credentials_path_db1 (str): Database 1 credentials path\n",
    "                    credentials_path_db1 (str): Databsae 2 credentials path\n",
    "                \"\"\"\n",
    "                os.environ['TNS_ADMIN'] = tns_admin_path\n",
    "                \n",
    "                # Load private key\n",
    "                key_key_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\key.key'\n",
    "                with open(key_key_path, \"rb\") as key_file:\n",
    "                    key = key_file.read()\n",
    "\n",
    "                cipher = Fernet(key)\n",
    "                \n",
    "                # Load encrypted data\n",
    "                encoded_env_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\.env.enc'\n",
    "                with open(encoded_env_path, \"rb\") as encrypted_file:\n",
    "                    encrypted_data = encrypted_file.read()\n",
    "\n",
    "                decrypted_data = cipher.decrypt(encrypted_data).decode()\n",
    "\n",
    "                env_file = StringIO(decrypted_data)\n",
    "                load_dotenv(stream=env_file)\n",
    "\n",
    "                self.username1 = os.getenv('main_username')\n",
    "                self.password1 = os.getenv('main_password')\n",
    "                self.dsn1 = os.getenv('main_dsn')\n",
    "\n",
    "                self.username2 = os.getenv('datamart_username')\n",
    "                self.password2 = os.getenv('datamart_password')\n",
    "                self.dsn2 = os.getenv('datamart_dsn')\n",
    "\n",
    "                self.connection_string1 = f'oracle+oracledb://{self.username1}:{self.password1}@{self.dsn1}'\n",
    "                self.connection_string2 = f'oracle+oracledb://{self.username2}:{self.password2}@{self.dsn2}'\n",
    "\n",
    "                self.engine1 = create_async_engine(self.connection_string1, max_identifier_length=128, echo=False, future=True)\n",
    "                self.engine1.dialect.hide_parameters = True\n",
    "                self.engine2 = create_async_engine(self.connection_string2, max_identifier_length=128, echo=False, future=True)\n",
    "                self.engine1.dialect.hide_parameters = True\n",
    "\n",
    "\n",
    "            async def query(self, sql_query, engine=1):\n",
    "                \"\"\"\n",
    "                This allows abstraction of the connection and the class\n",
    "                so the developer can query a single table as a dataframe\n",
    "\n",
    "                Args:\n",
    "                    sql_query (str): The query to SQL database is passed as a string\n",
    "                    engine (int): This selects the database. There are two engines:\n",
    "                        1 -> R1625\n",
    "                        2 -> COCC DataMart\n",
    "\n",
    "                Returns:\n",
    "                    df: The SQL query is returned as a pandas DataFrame\n",
    "\n",
    "                Usage:\n",
    "                    df = db_handler.query(\"SELECT * FROM DB.TABLE\", engine=1)\n",
    "\n",
    "                    In this example, db_handler = DatabaseHandler(args)\n",
    "                \"\"\"\n",
    "                if engine == 1:\n",
    "                    selected_engine = self.engine1\n",
    "                elif engine == 2:\n",
    "                    selected_engine = self.engine2\n",
    "                else:\n",
    "                    raise ValueError(\"Engine must be 1 or 2\")\n",
    "\n",
    "                async with selected_engine.connect() as connection:\n",
    "                    result = await connection.execute(sql_query)\n",
    "                    rows = result.fetchall()\n",
    "                    if not rows:\n",
    "                        return pd.DataFrame()\n",
    "                    df = pd.DataFrame(rows, columns=result.keys())\n",
    "                return df\n",
    "\n",
    "            async def close(self):\n",
    "                if self.engine1:\n",
    "                    await self.engine1.dispose()\n",
    "                if self.engine2:\n",
    "                    await self.engine2.dispose()\n",
    "\n",
    "\n",
    "        # Database Connection Configuration\n",
    "        tns_admin_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\tns_admin'\n",
    "        db_handler = DatabaseHandler(tns_admin_path)\n",
    "\n",
    "        async def fetch_data(queries):\n",
    "            try:\n",
    "                tasks = {query['key']: asyncio.create_task(db_handler.query(query['sql'], query['engine'])) for query in queries}\n",
    "                results = await asyncio.gather(*tasks.values())\n",
    "                return {key: df for key, df in zip(tasks.keys(), results)}\n",
    "            except Exception as e:\n",
    "                print(f\"Error\")\n",
    "                raise\n",
    "            finally:\n",
    "                await db_handler.close()\n",
    "\n",
    "        def run_sql_queries():\n",
    "            query = text(f\"\"\"\n",
    "                SELECT b.ACCTNBR, b.EFFDATE, b.BOOKBALANCE, c.CREDITLIMITAMT\n",
    "                FROM COCCDM.WH_ACCTCOMMON b\n",
    "                JOIN COCCDM.WH_ACCTLOAN c\n",
    "                ON b.ACCTNBR = c.ACCTNBR AND b.EFFDATE = c.EFFDATE\n",
    "                WHERE b.ACCTNBR IN ({acctnbr_placeholder})\n",
    "                AND b.EFFDATE BETWEEN TO_DATE('{year_ago_date}', 'yyyy-mm-dd hh24:mi:ss') AND TO_DATE('{current_date}', 'yyyy-mm-dd hh24:mi:ss')\n",
    "            \"\"\")\n",
    "\n",
    "            queries = [\n",
    "                {'key':'query', 'sql':query, 'engine':2},\n",
    "                # {'key':'acctcommon', 'sql':acctcommon, 'engine':1},\n",
    "                # {'key':'persaddruse', 'sql':persaddruse, 'engine':1},\n",
    "                # {'key':'orgaddruse', 'sql':orgaddruse, 'engine':1},\n",
    "                # {'key':'wh_addr', 'sql':wh_addr, 'engine':1},\n",
    "                # {'key':'wh_allroles', 'sql':wh_allroles, 'engine':1},\n",
    "            ]\n",
    "\n",
    "            async def run_queries():\n",
    "                return await fetch_data(queries)\n",
    "            \n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                return loop.run_until_complete(run_queries())\n",
    "            else:\n",
    "                return asyncio.run(run_queries())\n",
    "            \n",
    "        data = run_sql_queries()\n",
    "        \n",
    "        return data\n",
    "\n",
    "    data = retrieve_data()\n",
    "    df = data['query'].copy()\n",
    "    \n",
    "    df['bookbalance'] = pd.to_numeric(df['bookbalance'], errors='coerce')\n",
    "    df['creditlimitamt'] = pd.to_numeric(df['creditlimitamt'], errors='coerce')\n",
    "    \n",
    "    df['creditlimitamt'] = df['creditlimitamt'].fillna(0)\n",
    "    \n",
    "    df1 = df\n",
    "    df1['ttm line utilization'] = df1['bookbalance'] / df1['creditlimitamt']\n",
    "    df1['ttm line utilization'] = df1['ttm line utilization'].fillna(0)\n",
    "    df1['ttm line utilization'] = df1['ttm line utilization'].replace([np.inf], 1.00)\n",
    "    df1 = df1.groupby('acctnbr')['ttm line utilization'].mean().reset_index()\n",
    "    \n",
    "    df2 = df\n",
    "    df2['effdate'] = pd.to_datetime(df2['effdate'])\n",
    "    df2 = df2.sort_values(by=['acctnbr','effdate'])\n",
    "    \n",
    "    def check_30_day_cleanup(group):\n",
    "        group['rolling_zeros'] = group['bookbalance'].rolling(window=30).apply(lambda x: (x == 0).all(), raw=True)\n",
    "        return 0 if (group['rolling_zeros'] == 1).any() else 1\n",
    "    \n",
    "    df2 = df2.groupby('acctnbr').apply(check_30_day_cleanup, include_groups=False).reset_index(name='cleanup_provision')\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "#################################\n",
    "def append_line_utilization_data(loan_data, utilization_data, cleanup_data):\n",
    "    \"\"\"\n",
    "    Appends line utilization data to loan_data\n",
    "    \n",
    "    Args:\n",
    "        utilization_data: df with line utilization % over ttm\n",
    "        cleanup_data : df with 30-day cleanup test (boolean)\n",
    "        \n",
    "    Returns:\n",
    "        df: loan_data with additional tests\n",
    "        \n",
    "    Operations:\n",
    "        - left merge with acctnbr & utilization data on acctnbr\n",
    "        - left merge with acctnbr & cleanup_data on acctnbr\n",
    "    \"\"\"\n",
    "    df = pd.merge(loan_data, utilization_data, on='acctnbr', how='left')\n",
    "    df = pd.merge(df, cleanup_data, on='acctnbr', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "def get_inactive_date(acctloanlimithist):\n",
    "    \"\"\"\n",
    "    Getting inactive date for each item\n",
    "    \n",
    "    Args: \n",
    "        acctloanlimithist: ACCTLOANLIMITHIST table (COCC)\n",
    "        \n",
    "    Returns:\n",
    "        df: df with the most recent inactive date per product\n",
    "        \n",
    "    Operations:\n",
    "        - ensure inactivedate is a datetime field\n",
    "        - groupby acctnbr, take max inactive date\n",
    "    \"\"\"\n",
    "    acctloanlimithist['inactivedate'] = pd.to_datetime(acctloanlimithist['inactivedate'])\n",
    "    df = acctloanlimithist.groupby('acctnbr')['inactivedate'].max().reset_index()\n",
    "    \n",
    "    assert df['acctnbr'].duplicated().sum() == 0, \"There are duplicate acctnbrs\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "def append_inactive_date(loan_data, inactive_date_df):\n",
    "    \"\"\"\n",
    "    Append inactive date to loan_data\n",
    "    \n",
    "    Args:\n",
    "        loan_data: loan_data\n",
    "        inactive_date_df = df with the most recent inactive date per product\n",
    "    \n",
    "    Returns:\n",
    "        df: loan_data with inactive date appended\n",
    "    \n",
    "    Operations:\n",
    "        - left merge with loan_data & inactive_date on acctnbr\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.merge(loan_data, inactive_date_df, on='acctnbr', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "#################################\n",
    "def criteria_flags(loan_data):\n",
    "    \"\"\"\n",
    "    Criteria flags are assigned on to each line item for\n",
    "    identification of fails.\n",
    "    \n",
    "    Args:\n",
    "        loan_data\n",
    "        \n",
    "        # Parameters\n",
    "        ttm_pd_amt = 3\n",
    "        ttm_pd30_amt = 1\n",
    "        ttm_overdrafts = 5\n",
    "        deposit_change_pct = -.3\n",
    "        min_deposits = 250000\n",
    "        utilization_limit = .6\n",
    "        \n",
    "    Returns:\n",
    "        df: loan_data with new identifier flag columns\n",
    "            ['past_due_flag']\n",
    "            ['ttm_overdrafts_flag']\n",
    "            ['deposit_change_flag']\n",
    "            ['ttm_utilization_flag']\n",
    "            - 'cleanup_provision' already exists as a boolean column\n",
    "    \n",
    "    Operations:\n",
    "        - parameters are set\n",
    "        - if ttm_pd > parameter or ttm_pd30 >= parameter, then past_due_flag = 1, else 0\n",
    "        - if ttm_overdrafts >= parameter, then ttm_overdrafts_flag = 1, else 0\n",
    "        - if deposit_change_pct >= parameter, then deposit_change_flag = 1, else 0\n",
    "        - if ttm_line_utilization >= parameter, then ttm_utilization_flag = 1, else 0\n",
    "        - flag created for passing all tests (1: passed all, 0: failed at least 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters\n",
    "    ttm_pd_amt = 3\n",
    "    ttm_pd30_amt = 1\n",
    "    ttm_overdrafts = 5\n",
    "    deposit_change_pct = -.3\n",
    "    min_deposits = 250000\n",
    "    utilization_limit = .6\n",
    "    \n",
    "    # Flag Column Creation\n",
    "    loan_data['past_due_flag'] = np.where((loan_data['ttm_pd'] >= ttm_pd_amt) | (loan_data['ttm_pd30'] >= ttm_pd30_amt), 1, 0) \n",
    "    loan_data['ttm_overdrafts_flag'] = np.where((loan_data['TTM Overdrafts'] >= ttm_overdrafts), 1, 0)\n",
    "    loan_data['deposit_change_flag'] = np.where((loan_data['Deposit Change Pct'] <= deposit_change_pct) & (loan_data['Year Ago Balance'] >= min_deposits), 1, 0)\n",
    "    loan_data['ttm_utilization_flag'] = np.where((loan_data['ttm line utilization'] >= utilization_limit), 1, 0)\n",
    "    loan_data['passed_all_flag'] = np.where((loan_data['past_due_flag'] == 0) & (loan_data['ttm_overdrafts_flag'] == 0) & (loan_data['deposit_change_flag'] == 0) & (loan_data['ttm_utilization_flag'] == 0) & (loan_data['cleanup_provision'] == 0), 1, 0)\n",
    "    \n",
    "    return loan_data\n",
    "\n",
    "\n",
    "# # #################################\n",
    "# def main():\n",
    "\n",
    "    # Database Connection Configuration\n",
    "data = retrieve_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acctcommon = data['acctcommon'].copy()\n",
    "acctloan = data['acctloan'].copy()\n",
    "loans = data['loans'].copy()\n",
    "househldacct = data['househldacct'].copy()\n",
    "\n",
    "# Data for Xactus\n",
    "allroles = data['allroles'].copy()\n",
    "persaddruse = data['persaddruse'].copy()\n",
    "wh_addr = data['wh_addr'].copy()\n",
    "pers = data['pers'].copy()\n",
    "\n",
    "acctstatistichist = data['acctstatistichist'].copy()\n",
    "acctloanlimithist = data['acctloanlimithist'].copy()\n",
    "\n",
    "# Core ETL\n",
    "loan_data = filter_and_merge_loan_tables(acctcommon, acctloan, loans)\n",
    "loan_data = append_total_exposure_field(loan_data)\n",
    "househldacct = drop_hh_duplicates(househldacct)\n",
    "loan_data = append_household_number(loan_data, househldacct)\n",
    "household_grouping_df = household_total_exposure(loan_data)\n",
    "loan_data = append_household_exposure(loan_data, household_grouping_df)\n",
    "loan_data = filter_to_target_products(loan_data)\n",
    "acctstatistic_output = acctstatistichist_cleaning(acctstatistichist, acctcommon)\n",
    "pd_df = count_pd(acctstatistic_output)\n",
    "pd30_df = count_pd30(acctstatistic_output)\n",
    "loan_data = append_pd_info(loan_data, pd_df, pd30_df)\n",
    "deposit_data = deposit_criteria_testing()\n",
    "loan_data = append_deposit_data(loan_data, deposit_data)\n",
    "utilization_data, cleanup_data = line_utilization_fetch(loan_data)\n",
    "loan_data = append_line_utilization_data(loan_data, utilization_data, cleanup_data)\n",
    "inactive_date_df = get_inactive_date(acctloanlimithist)\n",
    "loan_data = append_inactive_date(loan_data, inactive_date_df)\n",
    "loan_data = criteria_flags(loan_data)\n",
    "\n",
    "# # Consolidation of the columns necessary\n",
    "# final_df = loan_data[['acctnbr','effdate','ownername','product','loanofficer','inactivedate','Net Balance','Net Available','Net Collateral Reserve','cobal','creditlimitamt','Total Exposure_hhgroup','ttm_pd','ttm_pd30','TTM Overdrafts','NOTEBAL','Year Ago Balance','Deposit Change Pct','ttm line utilization','cleanup_provision','riskratingcd','past_due_flag','ttm_overdrafts_flag','deposit_change_flag','ttm_utilization_flag','passed_all_flag']]\n",
    "\n",
    "# # Writing output\n",
    "# file_path = r'\\\\10.161.85.66\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Alerts\\Production\\Output\\alerts.xlsx'\n",
    "# final_df.to_excel(file_path, index=False, engine='openpyxl')\n",
    "\n",
    "# # Produce Documentation (docstrings)\n",
    "# documentation_list = [\n",
    "#                         filter_and_merge_loan_tables, \n",
    "#                         append_total_exposure_field, \n",
    "#                         append_household_number, \n",
    "#                         household_total_exposure, \n",
    "#                         append_household_exposure, \n",
    "#                         filter_to_target_products,\n",
    "#                         acctstatistichist_cleaning,\n",
    "#                         count_pd,\n",
    "#                         count_pd30,\n",
    "#                         append_pd_info,\n",
    "#                         deposit_criteria_testing,\n",
    "#                         append_deposit_data,\n",
    "#                         line_utilization_fetch,\n",
    "#                         append_line_utilization_data,\n",
    "#                         get_inactive_date,\n",
    "#                         append_inactive_date,\n",
    "#                         criteria_flags]\n",
    "\n",
    "# # for i in documentation_list:\n",
    "# #     print(help(i))\n",
    "# #     print(\"\")\n",
    "    \n",
    "# print('Execution Complete!')\n",
    "    \n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()\n",
    "\n",
    "    \n",
    "# # main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data for Xactus\n",
    "# allroles = data['allroles'].copy()\n",
    "# persaddruse = data['persaddruse'].copy()\n",
    "# wh_addr = data['wh_addr'].copy()\n",
    "# pers = data['pers'].copy()\n",
    "################################\n",
    "# # Personal Guarantor\n",
    "pg_section = personal_guarantors(allroles, persaddruse, wh_addr, pers)\n",
    "target_loans_with_guar = merge_guar_with_loan_data(loan_data, pg_section)\n",
    "\n",
    "# assert len(target_loans_with_guar) > 0, \"There are no records\"\n",
    "# xactus_extract = target_loans_with_guar[['acctnbr','persnbr_pg','firstname','lastname','text1','cityname','statecd','zipcd']]\n",
    "# file_path = r'Y:\\GlobalWave\\CLO Intern Deliverables\\070824PFS_Check_v2.xlsx'\n",
    "# colin_list = pd.read_excel(file_path, engine='openpyxl')\n",
    "# colin_list = colin_list.astype({\"Person Number\": float})\n",
    "# merged_df = pd.merge(xactus_extract, colin_list, how='left', left_on='persnbr_pg', right_on='Person Number', suffixes=('_xactus','_colin'), indicator=True)\n",
    "# merged_df.groupby('_merge')['persnbr_pg'].count()\n",
    "# # Output Guarantor Data\n",
    "# file_name = r'\\\\10.161.85.66\\Home\\Share\\Line of Business_Shared Services\\Commercial Credit\\CML_Executive_Leadership_Projects\\Alerts\\Xactus\\Data\\guarantor_data.xlsx'\n",
    "# merged_df.to_excel(file_name, index=False)\n",
    "### Pending external action on the Xactus SFTP setup & information regarding permission to run soft pull credit scores\n",
    "### Will continue developing the other tests\n",
    "# target_loans_with_guar.info(verbose=True, null_counts=True)\n",
    "\n",
    "# # Initializing Database for Xactus\n",
    "# file_path = r'\\\\10.161.85.66\\Home\\Share\\Line of Business_Shared Services\\Commercial Credit\\CML_Executive_Leadership_Projects\\Alerts\\Xactus\\Database\\temporary.db'\n",
    "# engine = create_engine(f'sqlite:///{file_path}')\n",
    "\n",
    "# with engine.connect() as connection:\n",
    "#     connection.execute(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS guarantors (\n",
    "#         acctnbr INTEGER,\n",
    "#         persnbr INTEGER,\n",
    "#         firstname TEXT,\n",
    "#         lastname TEXT,\n",
    "#         creditscore INTEGER,\n",
    "#         effdate DATETIME\n",
    "#     );\n",
    "#     \"\"\")\n",
    "    \n",
    "# with engine.connect() as connection:\n",
    "#     data = pd.read_sql(\"SELECT * FROM guarantors\", connection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 302 entries, 0 to 301\n",
      "Data columns (total 46 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   acctnbr                 302 non-null    int64         \n",
      " 1   effdate                 302 non-null    datetime64[ns]\n",
      " 2   mjaccttypcd             302 non-null    object        \n",
      " 3   product                 302 non-null    object        \n",
      " 4   currmiaccttypcd         302 non-null    object        \n",
      " 5   bookbalance             302 non-null    object        \n",
      " 6   loanofficer             302 non-null    object        \n",
      " 7   ownername               302 non-null    object        \n",
      " 8   curracctstatcd          302 non-null    object        \n",
      " 9   contractdate            302 non-null    datetime64[ns]\n",
      " 10  notebal                 302 non-null    object        \n",
      " 11  cobal                   302 non-null    object        \n",
      " 12  creditlimitamt          302 non-null    object        \n",
      " 13  riskratingcd            302 non-null    object        \n",
      " 14  totalpctsold            302 non-null    object        \n",
      " 15  credlimitclatresamt     302 non-null    object        \n",
      " 16  availbalamt             302 non-null    object        \n",
      " 17  Net Balance             302 non-null    object        \n",
      " 18  Net Available           302 non-null    object        \n",
      " 19  Net Collateral Reserve  302 non-null    object        \n",
      " 20  Total Exposure_df       302 non-null    object        \n",
      " 21  householdnbr            302 non-null    float64       \n",
      " 22  Total Exposure_hhgroup  302 non-null    object        \n",
      " 23  ytd_pd                  302 non-null    float64       \n",
      " 24  ttm_pd                  302 non-null    float64       \n",
      " 25  ytd_pd30                302 non-null    float64       \n",
      " 26  ttm_pd30                302 non-null    float64       \n",
      " 27  NOTEBAL                 295 non-null    float64       \n",
      " 28  Year Ago Balance        295 non-null    float64       \n",
      " 29  TTM Overdrafts          295 non-null    float64       \n",
      " 30  Deposit Change Pct      295 non-null    float64       \n",
      " 31  ttm line utilization    302 non-null    float64       \n",
      " 32  cleanup_provision       302 non-null    int64         \n",
      " 33  inactivedate            302 non-null    datetime64[ns]\n",
      " 34  past_due_flag           302 non-null    int64         \n",
      " 35  ttm_overdrafts_flag     302 non-null    int64         \n",
      " 36  deposit_change_flag     302 non-null    int64         \n",
      " 37  ttm_utilization_flag    302 non-null    int64         \n",
      " 38  passed_all_flag         302 non-null    int64         \n",
      " 39  persnbr                 302 non-null    float64       \n",
      " 40  firstname               302 non-null    object        \n",
      " 41  lastname                302 non-null    object        \n",
      " 42  text1                   301 non-null    object        \n",
      " 43  cityname                301 non-null    object        \n",
      " 44  statecd                 301 non-null    object        \n",
      " 45  zipcd                   301 non-null    object        \n",
      "dtypes: datetime64[ns](3), float64(11), int64(7), object(25)\n",
      "memory usage: 108.7+ KB\n"
     ]
    }
   ],
   "source": [
    "target_loans_with_guar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(target_loans_with_guar) > 0, \"There are no records\"\n",
    "xactus_extract = target_loans_with_guar[['acctnbr','persnbr','firstname','lastname','text1','cityname','statecd','zipcd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acctnbr</th>\n",
       "      <th>persnbr</th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>text1</th>\n",
       "      <th>cityname</th>\n",
       "      <th>statecd</th>\n",
       "      <th>zipcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150304683</td>\n",
       "      <td>421.0</td>\n",
       "      <td>NANCY</td>\n",
       "      <td>DASILVA</td>\n",
       "      <td>2265 WHEELER ST</td>\n",
       "      <td>NORTH DIGHTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150304683</td>\n",
       "      <td>1027703.0</td>\n",
       "      <td>BRIAN</td>\n",
       "      <td>DASILVA</td>\n",
       "      <td>2265 WHEELER ST</td>\n",
       "      <td>NORTH DIGHTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150498569</td>\n",
       "      <td>1032720.0</td>\n",
       "      <td>WALTER</td>\n",
       "      <td>ARAUJO</td>\n",
       "      <td>23 FAIRWAY DR</td>\n",
       "      <td>ACUSHNET</td>\n",
       "      <td>MA</td>\n",
       "      <td>02743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150498569</td>\n",
       "      <td>1039104.0</td>\n",
       "      <td>MARINA</td>\n",
       "      <td>ARAUJO</td>\n",
       "      <td>23 FAIRWAY DR</td>\n",
       "      <td>ACUSHNET</td>\n",
       "      <td>MA</td>\n",
       "      <td>02743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150894551</td>\n",
       "      <td>1154368.0</td>\n",
       "      <td>NATHAN</td>\n",
       "      <td>RAY</td>\n",
       "      <td>135 ANGELL RD</td>\n",
       "      <td>CUMBERLAND</td>\n",
       "      <td>RI</td>\n",
       "      <td>02864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>400261381666</td>\n",
       "      <td>1067999.0</td>\n",
       "      <td>JAMES</td>\n",
       "      <td>HUTZLER</td>\n",
       "      <td>53 WATER WAY</td>\n",
       "      <td>BARRINGTON</td>\n",
       "      <td>RI</td>\n",
       "      <td>02806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>151062743</td>\n",
       "      <td>1105528.0</td>\n",
       "      <td>STEPHEN</td>\n",
       "      <td>MURPHY</td>\n",
       "      <td>62 WEST ST</td>\n",
       "      <td>CARVER</td>\n",
       "      <td>MA</td>\n",
       "      <td>02330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>100277881823</td>\n",
       "      <td>1031226.0</td>\n",
       "      <td>PETER</td>\n",
       "      <td>BARTEL</td>\n",
       "      <td>260 BURT ST</td>\n",
       "      <td>TAUNTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>02780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>150358052</td>\n",
       "      <td>1057148.0</td>\n",
       "      <td>MICHAEL</td>\n",
       "      <td>GALLAGHER</td>\n",
       "      <td>277 OAK ST</td>\n",
       "      <td>RAYNHAM</td>\n",
       "      <td>MA</td>\n",
       "      <td>02767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>500187481374</td>\n",
       "      <td>1068050.0</td>\n",
       "      <td>HERMAN</td>\n",
       "      <td>KROBATH</td>\n",
       "      <td>55 BENJAMIN DAY DR</td>\n",
       "      <td>WRENTHAM</td>\n",
       "      <td>MA</td>\n",
       "      <td>02093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          acctnbr    persnbr firstname   lastname               text1  \\\n",
       "0       150304683      421.0     NANCY    DASILVA     2265 WHEELER ST   \n",
       "1       150304683  1027703.0     BRIAN    DASILVA     2265 WHEELER ST   \n",
       "2       150498569  1032720.0    WALTER     ARAUJO       23 FAIRWAY DR   \n",
       "3       150498569  1039104.0    MARINA     ARAUJO       23 FAIRWAY DR   \n",
       "4       150894551  1154368.0    NATHAN        RAY       135 ANGELL RD   \n",
       "..            ...        ...       ...        ...                 ...   \n",
       "297  400261381666  1067999.0     JAMES    HUTZLER        53 WATER WAY   \n",
       "298     151062743  1105528.0   STEPHEN     MURPHY          62 WEST ST   \n",
       "299  100277881823  1031226.0     PETER     BARTEL         260 BURT ST   \n",
       "300     150358052  1057148.0   MICHAEL  GALLAGHER          277 OAK ST   \n",
       "301  500187481374  1068050.0    HERMAN    KROBATH  55 BENJAMIN DAY DR   \n",
       "\n",
       "          cityname statecd  zipcd  \n",
       "0    NORTH DIGHTON      MA  02764  \n",
       "1    NORTH DIGHTON      MA  02764  \n",
       "2         ACUSHNET      MA  02743  \n",
       "3         ACUSHNET      MA  02743  \n",
       "4       CUMBERLAND      RI  02864  \n",
       "..             ...     ...    ...  \n",
       "297     BARRINGTON      RI  02806  \n",
       "298         CARVER      MA  02330  \n",
       "299        TAUNTON      MA  02780  \n",
       "300        RAYNHAM      MA  02767  \n",
       "301       WRENTHAM      MA  02093  \n",
       "\n",
       "[302 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xactus_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xactus_extract = xactus_extract.drop_duplicates(subset='persnbr', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xactus_extract['persnbr'].is_unique, \"Duplicates found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Alerts\\Production\\Xactus\\xactus_extract.xlsx'\n",
    "# xactus_extract.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 288 entries, 0 to 301\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   acctnbr    288 non-null    int64  \n",
      " 1   persnbr    288 non-null    float64\n",
      " 2   firstname  288 non-null    object \n",
      " 3   lastname   288 non-null    object \n",
      " 4   text1      287 non-null    object \n",
      " 5   cityname   287 non-null    object \n",
      " 6   statecd    287 non-null    object \n",
      " 7   zipcd      287 non-null    object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 20.2+ KB\n"
     ]
    }
   ],
   "source": [
    "xactus_extract.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'Z:\\Chad Projects\\Alerts\\Production\\Xactus\\Permission_db\\pfs_permission.csv'\n",
    "pers_permission_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persnbr</th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>Permission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>327.0</td>\n",
       "      <td>DONALD</td>\n",
       "      <td>SMYTH</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>421.0</td>\n",
       "      <td>NANCY</td>\n",
       "      <td>DASILVA</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1012786.0</td>\n",
       "      <td>MICHAEL</td>\n",
       "      <td>BRIGGS</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021325.0</td>\n",
       "      <td>PAMELA</td>\n",
       "      <td>DUMAS</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1025471.0</td>\n",
       "      <td>TIMOTHY</td>\n",
       "      <td>DUBUC</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1165744.0</td>\n",
       "      <td>CARLOS</td>\n",
       "      <td>COSTA</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1165801.0</td>\n",
       "      <td>JOSHUA</td>\n",
       "      <td>ABREU</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1166597.0</td>\n",
       "      <td>JULIE</td>\n",
       "      <td>EISENHAUER</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1166896.0</td>\n",
       "      <td>OWEN</td>\n",
       "      <td>DOYLE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1167037.0</td>\n",
       "      <td>TRAVIS</td>\n",
       "      <td>GERVASIO</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       persnbr firstname    lastname Permission\n",
       "0        327.0    DONALD       SMYTH          Y\n",
       "1        421.0     NANCY     DASILVA          Y\n",
       "2    1012786.0   MICHAEL      BRIGGS          Y\n",
       "3    1021325.0    PAMELA       DUMAS          Y\n",
       "4    1025471.0   TIMOTHY       DUBUC          Y\n",
       "..         ...       ...         ...        ...\n",
       "271  1165744.0    CARLOS       COSTA          Y\n",
       "272  1165801.0    JOSHUA       ABREU          Y\n",
       "273  1166597.0     JULIE  EISENHAUER          Y\n",
       "274  1166896.0      OWEN       DOYLE          Y\n",
       "275  1167037.0    TRAVIS    GERVASIO          Y\n",
       "\n",
       "[276 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pers_permission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_permission_df = pers_permission_df.loc[:,['persnbr','Permission']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pers_permission_df['persnbr'].is_unique, \"Duplicates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(xactus_extract, pers_permission_df, how='inner', on='persnbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 276 entries, 0 to 275\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   acctnbr     276 non-null    int64  \n",
      " 1   persnbr     276 non-null    float64\n",
      " 2   firstname   276 non-null    object \n",
      " 3   lastname    276 non-null    object \n",
      " 4   text1       275 non-null    object \n",
      " 5   cityname    275 non-null    object \n",
      " 6   statecd     275 non-null    object \n",
      " 7   zipcd       275 non-null    object \n",
      " 8   Permission  276 non-null    object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 19.5+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acctnbr</th>\n",
       "      <th>persnbr</th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>text1</th>\n",
       "      <th>cityname</th>\n",
       "      <th>statecd</th>\n",
       "      <th>zipcd</th>\n",
       "      <th>Permission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150304683</td>\n",
       "      <td>421.0</td>\n",
       "      <td>NANCY</td>\n",
       "      <td>DASILVA</td>\n",
       "      <td>2265 WHEELER ST</td>\n",
       "      <td>NORTH DIGHTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>02764</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150304683</td>\n",
       "      <td>1027703.0</td>\n",
       "      <td>BRIAN</td>\n",
       "      <td>DASILVA</td>\n",
       "      <td>2265 WHEELER ST</td>\n",
       "      <td>NORTH DIGHTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>02764</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>102807</td>\n",
       "      <td>1067980.0</td>\n",
       "      <td>CARLOS</td>\n",
       "      <td>DASILVA</td>\n",
       "      <td>17 CLARK RD</td>\n",
       "      <td>SMITHFIELD</td>\n",
       "      <td>RI</td>\n",
       "      <td>02917</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acctnbr    persnbr firstname lastname            text1       cityname  \\\n",
       "0    150304683      421.0     NANCY  DASILVA  2265 WHEELER ST  NORTH DIGHTON   \n",
       "1    150304683  1027703.0     BRIAN  DASILVA  2265 WHEELER ST  NORTH DIGHTON   \n",
       "149     102807  1067980.0    CARLOS  DASILVA      17 CLARK RD     SMITHFIELD   \n",
       "\n",
       "    statecd  zipcd Permission  \n",
       "0        MA  02764          Y  \n",
       "1        MA  02764          Y  \n",
       "149      RI  02917          Y  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[merged_df['lastname'].str.contains('dasilva',case=False,na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[~(merged_df['persnbr'].isin([421, 1027703]))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 274 entries, 2 to 275\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   acctnbr     274 non-null    int64  \n",
      " 1   persnbr     274 non-null    float64\n",
      " 2   firstname   274 non-null    object \n",
      " 3   lastname    274 non-null    object \n",
      " 4   text1       273 non-null    object \n",
      " 5   cityname    273 non-null    object \n",
      " 6   statecd     273 non-null    object \n",
      " 7   zipcd       273 non-null    object \n",
      " 8   Permission  274 non-null    object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 21.4+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[~(merged_df['text1'].isnull())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 273 entries, 2 to 275\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   acctnbr     273 non-null    int64  \n",
      " 1   persnbr     273 non-null    float64\n",
      " 2   firstname   273 non-null    object \n",
      " 3   lastname    273 non-null    object \n",
      " 4   text1       273 non-null    object \n",
      " 5   cityname    273 non-null    object \n",
      " 6   statecd     273 non-null    object \n",
      " 7   zipcd       273 non-null    object \n",
      " 8   Permission  273 non-null    object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 21.3+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async Connector\n",
    "# Developed by CD\n",
    "\n",
    "from io import StringIO\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta, date\n",
    "from sqlalchemy.ext.asyncio import create_async_engine\n",
    "from sqlalchemy import text\n",
    "from typing import List\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from cryptography.fernet import Fernet\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import sys\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "\n",
    "def retrieve_data():\n",
    "    \"\"\"\n",
    "    Retrieve data from COCC database\n",
    "    \"\"\"\n",
    "    class DatabaseHandler:\n",
    "        \"\"\"\n",
    "        This class abstracts the connection to the database and allows a clean\n",
    "        interface for the developer to use.\n",
    "\n",
    "        This connector can handle async queries\n",
    "\n",
    "        \"\"\"\n",
    "        def __init__(self, tns_admin_path):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                tns_admin_path (str): Oracle driver path\n",
    "                credentials_path_db1 (str): Database 1 credentials path\n",
    "                credentials_path_db1 (str): Databsae 2 credentials path\n",
    "            \"\"\"\n",
    "            os.environ['TNS_ADMIN'] = tns_admin_path\n",
    "            \n",
    "            # Load private key\n",
    "            key_key_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\key.key'\n",
    "            with open(key_key_path, \"rb\") as key_file:\n",
    "                key = key_file.read()\n",
    "\n",
    "            cipher = Fernet(key)\n",
    "            \n",
    "            # Load encrypted data\n",
    "            encoded_env_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\.env.enc'\n",
    "            with open(encoded_env_path, \"rb\") as encrypted_file:\n",
    "                encrypted_data = encrypted_file.read()\n",
    "\n",
    "            decrypted_data = cipher.decrypt(encrypted_data).decode()\n",
    "\n",
    "            env_file = StringIO(decrypted_data)\n",
    "            load_dotenv(stream=env_file)\n",
    "\n",
    "            self.username1 = os.getenv('main_username')\n",
    "            self.password1 = os.getenv('main_password')\n",
    "            self.dsn1 = os.getenv('main_dsn')\n",
    "\n",
    "            self.username2 = os.getenv('datamart_username')\n",
    "            self.password2 = os.getenv('datamart_password')\n",
    "            self.dsn2 = os.getenv('datamart_dsn')\n",
    "\n",
    "            self.connection_string1 = f'oracle+oracledb://{self.username1}:{self.password1}@{self.dsn1}'\n",
    "            self.connection_string2 = f'oracle+oracledb://{self.username2}:{self.password2}@{self.dsn2}'\n",
    "\n",
    "            self.engine1 = create_async_engine(self.connection_string1, max_identifier_length=128, echo=False, future=True)\n",
    "            self.engine1.dialect.hide_parameters = True\n",
    "            self.engine2 = create_async_engine(self.connection_string2, max_identifier_length=128, echo=False, future=True)\n",
    "            self.engine1.dialect.hide_parameters = True\n",
    "\n",
    "\n",
    "        async def query(self, sql_query, engine=1):\n",
    "            \"\"\"\n",
    "            This allows abstraction of the connection and the class\n",
    "            so the developer can query a single table as a dataframe\n",
    "\n",
    "            Args:\n",
    "                sql_query (str): The query to SQL database is passed as a string\n",
    "                engine (int): This selects the database. There are two engines:\n",
    "                    1 -> R1625\n",
    "                    2 -> COCC DataMart\n",
    "\n",
    "            Returns:\n",
    "                df: The SQL query is returned as a pandas DataFrame\n",
    "\n",
    "            Usage:\n",
    "                df = db_handler.query(\"SELECT * FROM DB.TABLE\", engine=1)\n",
    "\n",
    "                In this example, db_handler = DatabaseHandler(args)\n",
    "            \"\"\"\n",
    "            if engine == 1:\n",
    "                selected_engine = self.engine1\n",
    "            elif engine == 2:\n",
    "                selected_engine = self.engine2\n",
    "            else:\n",
    "                raise ValueError(\"Engine must be 1 or 2\")\n",
    "\n",
    "            async with selected_engine.connect() as connection:\n",
    "                result = await connection.execute(sql_query)\n",
    "                rows = result.fetchall()\n",
    "                if not rows:\n",
    "                    return pd.DataFrame()\n",
    "                df = pd.DataFrame(rows, columns=result.keys())\n",
    "            return df\n",
    "\n",
    "        async def close(self):\n",
    "            if self.engine1:\n",
    "                await self.engine1.dispose()\n",
    "            if self.engine2:\n",
    "                await self.engine2.dispose()\n",
    "\n",
    "\n",
    "    # Database Connection Configuration\n",
    "    tns_admin_path = r'\\\\00-da1\\Home\\Share\\Data & Analytics Initiatives\\Project Management\\Chad Projects\\Utility\\env_admin\\tns_admin'\n",
    "    db_handler = DatabaseHandler(tns_admin_path)\n",
    "\n",
    "    async def fetch_data(queries):\n",
    "        try:\n",
    "            tasks = {query['key']: asyncio.create_task(db_handler.query(query['sql'], query['engine'])) for query in queries}\n",
    "            results = await asyncio.gather(*tasks.values())\n",
    "            return {key: df for key, df in zip(tasks.keys(), results)}\n",
    "        except Exception as e:\n",
    "            print(f\"Error\")\n",
    "            raise\n",
    "        finally:\n",
    "            await db_handler.close()\n",
    "\n",
    "    def run_sql_queries():\n",
    "        viewperstaxid = text(\"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM \n",
    "            OSIBANK.VIEWPERSTAXID\n",
    "        \"\"\")\n",
    "\n",
    "        queries = [\n",
    "            # {'key':'acctcommon', 'sql':acctcommon, 'engine':2},\n",
    "            {'key':'viewperstaxid', 'sql':viewperstaxid, 'engine':1},\n",
    "            # {'key':'loans', 'sql':loans, 'engine':1},\n",
    "            # {'key':'househldacct', 'sql':househldacct, 'engine':1},\n",
    "            # {'key':'allroles', 'sql':allroles, 'engine':1},\n",
    "            # {'key':'persaddruse', 'sql':persaddruse, 'engine':1},\n",
    "            # {'key':'wh_addr', 'sql':wh_addr, 'engine':1},\n",
    "            # {'key':'pers', 'sql':pers, 'engine':1},\n",
    "            # {'key':'acctstatistichist', 'sql':acctstatistichist, 'engine':1},\n",
    "            # {'key':'acctloanlimithist', 'sql':acctloanlimithist, 'engine':1},\n",
    "        ]\n",
    "\n",
    "        async def run_queries():\n",
    "            return await fetch_data(queries)\n",
    "        \n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            return loop.run_until_complete(run_queries())\n",
    "        else:\n",
    "            return asyncio.run(run_queries())\n",
    "        \n",
    "    data = run_sql_queries()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewperstaxid = data['viewperstaxid'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert viewperstaxid['persnbr'].is_unique, \"Duplicates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_with_ssn = pd.merge(merged_df, viewperstaxid, on='persnbr', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acctnbr</th>\n",
       "      <th>persnbr</th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>text1</th>\n",
       "      <th>cityname</th>\n",
       "      <th>statecd</th>\n",
       "      <th>zipcd</th>\n",
       "      <th>Permission</th>\n",
       "      <th>taxid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150498569</td>\n",
       "      <td>1032720.0</td>\n",
       "      <td>WALTER</td>\n",
       "      <td>ARAUJO</td>\n",
       "      <td>23 FAIRWAY DR</td>\n",
       "      <td>ACUSHNET</td>\n",
       "      <td>MA</td>\n",
       "      <td>02743</td>\n",
       "      <td>Y</td>\n",
       "      <td>038522937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150498569</td>\n",
       "      <td>1039104.0</td>\n",
       "      <td>MARINA</td>\n",
       "      <td>ARAUJO</td>\n",
       "      <td>23 FAIRWAY DR</td>\n",
       "      <td>ACUSHNET</td>\n",
       "      <td>MA</td>\n",
       "      <td>02743</td>\n",
       "      <td>Y</td>\n",
       "      <td>025548474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150894551</td>\n",
       "      <td>1154368.0</td>\n",
       "      <td>NATHAN</td>\n",
       "      <td>RAY</td>\n",
       "      <td>135 ANGELL RD</td>\n",
       "      <td>CUMBERLAND</td>\n",
       "      <td>RI</td>\n",
       "      <td>02864</td>\n",
       "      <td>Y</td>\n",
       "      <td>035644833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150664441</td>\n",
       "      <td>1107236.0</td>\n",
       "      <td>JOHNNY</td>\n",
       "      <td>CORDEIRO</td>\n",
       "      <td>32 REEVES ST</td>\n",
       "      <td>FALL RIVER</td>\n",
       "      <td>MA</td>\n",
       "      <td>02721</td>\n",
       "      <td>Y</td>\n",
       "      <td>011708277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150664441</td>\n",
       "      <td>1115852.0</td>\n",
       "      <td>OTTO</td>\n",
       "      <td>SCHLEINKOFER</td>\n",
       "      <td>10 HARVEST LN</td>\n",
       "      <td>BERKLEY</td>\n",
       "      <td>MA</td>\n",
       "      <td>02779</td>\n",
       "      <td>Y</td>\n",
       "      <td>046687767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     acctnbr    persnbr firstname      lastname          text1    cityname  \\\n",
       "0  150498569  1032720.0    WALTER        ARAUJO  23 FAIRWAY DR    ACUSHNET   \n",
       "1  150498569  1039104.0    MARINA        ARAUJO  23 FAIRWAY DR    ACUSHNET   \n",
       "2  150894551  1154368.0    NATHAN           RAY  135 ANGELL RD  CUMBERLAND   \n",
       "3  150664441  1107236.0    JOHNNY      CORDEIRO   32 REEVES ST  FALL RIVER   \n",
       "4  150664441  1115852.0      OTTO  SCHLEINKOFER  10 HARVEST LN     BERKLEY   \n",
       "\n",
       "  statecd  zipcd Permission      taxid  \n",
       "0      MA  02743          Y  038522937  \n",
       "1      MA  02743          Y  025548474  \n",
       "2      RI  02864          Y  035644833  \n",
       "3      MA  02721          Y  011708277  \n",
       "4      MA  02779          Y  046687767  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_with_ssn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pers_permission_df = merged_df_with_ssn.loc[:, ['persnbr','firstname','lastname']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pers_permission_df['Permission'] = 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_extract = merged_df_with_ssn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_extract['Reference Number'] = None\n",
    "# cleaned_extract['SSN'] = None\n",
    "cleaned_extract['Address Number'] = None\n",
    "cleaned_extract['Pre-Directional'] = None\n",
    "cleaned_extract['Post-Directional'] = None\n",
    "cleaned_extract['Street Type'] = None\n",
    "cleaned_extract['Maternal Name'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 273 entries, 0 to 272\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   acctnbr           273 non-null    int64  \n",
      " 1   persnbr           273 non-null    float64\n",
      " 2   firstname         273 non-null    object \n",
      " 3   lastname          273 non-null    object \n",
      " 4   text1             273 non-null    object \n",
      " 5   cityname          273 non-null    object \n",
      " 6   statecd           273 non-null    object \n",
      " 7   zipcd             273 non-null    object \n",
      " 8   Permission        273 non-null    object \n",
      " 9   taxid             273 non-null    object \n",
      " 10  Reference Number  0 non-null      object \n",
      " 11  Address Number    0 non-null      object \n",
      " 12  Pre-Directional   0 non-null      object \n",
      " 13  Post-Directional  0 non-null      object \n",
      " 14  Street Type       0 non-null      object \n",
      " 15  Maternal Name     0 non-null      object \n",
      "dtypes: float64(1), int64(1), object(14)\n",
      "memory usage: 34.3+ KB\n"
     ]
    }
   ],
   "source": [
    "cleaned_extract.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_extract = cleaned_extract.loc[:, ['firstname','lastname','Reference Number','taxid','Address Number','Pre-Directional','text1','Post-Directional','Street Type','cityname','statecd','zipcd','Maternal Name']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>Reference Number</th>\n",
       "      <th>taxid</th>\n",
       "      <th>Address Number</th>\n",
       "      <th>Pre-Directional</th>\n",
       "      <th>text1</th>\n",
       "      <th>Post-Directional</th>\n",
       "      <th>Street Type</th>\n",
       "      <th>cityname</th>\n",
       "      <th>statecd</th>\n",
       "      <th>zipcd</th>\n",
       "      <th>Maternal Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SHAYNA</td>\n",
       "      <td>STEIN</td>\n",
       "      <td>None</td>\n",
       "      <td>521850834</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>436 MORRIS AVE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PROVIDENCE</td>\n",
       "      <td>RI</td>\n",
       "      <td>02906</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   firstname lastname Reference Number      taxid Address Number  \\\n",
       "11    SHAYNA    STEIN             None  521850834           None   \n",
       "\n",
       "   Pre-Directional           text1 Post-Directional Street Type    cityname  \\\n",
       "11            None  436 MORRIS AVE             None        None  PROVIDENCE   \n",
       "\n",
       "   statecd  zipcd Maternal Name  \n",
       "11      RI  02906          None  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_extract[final_extract['firstname'].str.contains('shayna',case=False,na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_extract_with_persnbr = cleaned_extract.loc[:, ['firstname','lastname','Reference Number','taxid','Address Number','Pre-Directional','text1','Post-Directional','Street Type','cityname','statecd','zipcd','Maternal Name','persnbr']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'Z:\\Chad Projects\\Alerts\\Production\\Xactus\\Extract_Xactus\\final_extract_with_persnbr.xlsx'\n",
    "final_extract_with_persnbr.to_excel(file_path, index=False)\n",
    "\n",
    "file_path = r'Z:\\Chad Projects\\Alerts\\Production\\Xactus\\Extract_Xactus\\final_extract.xlsx'\n",
    "final_extract.to_excel(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete, for now. Will streamline this process for future use. Still a bit manual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
